{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e8a6f5",
   "metadata": {},
   "source": [
    "Реализуем градиентный спуск для задачи поиска оптимальных коэффициентов в MSE регрессии!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561e2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b0f0e0",
   "metadata": {},
   "source": [
    "Имеем 1000 объектов и 10 признаков у каждого (+таргет)!\n",
    "\n",
    "Обучим модель линейной регрессии:\n",
    "\n",
    "$$\n",
    "a(x) = \\beta_1 d_{1} + \\beta_2 d_{2} + \\beta_3 d_{3} + \\beta_4 d_{4} + \\beta_5 d_{5} + \\beta_6 d_{6} + \\beta_7 d_{7} + \\beta_8 d_{8} + \\beta_9 d_{9} + \\beta_{10} d_{10} + \\beta_0\n",
    "$$\n",
    "\n",
    "Которая минимизирует MSE:\n",
    "\n",
    "$$\n",
    "Q(a(X), Y) = \\sum_i^{1000} (a(x_i) - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb838abe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8251b",
   "metadata": {},
   "source": [
    "Обучим коэффициенты линейной регрессии с помощью библиотеки <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\"> **sklearn** </a>\n",
    "\n",
    "Отдельно выведем оценку свободного коэффициента  ($\\beta_0$ при $d_0 = 1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89990a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "### Your code is here\n",
    "model = LinearRegression()\n",
    "X = data.drop(['target'], axis=1)\n",
    "Y = data['target']\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037134f0-b12b-4bec-832b-001c9b0607db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_1 0.9999999999999971\n",
      "feature_2 1.9999999999999978\n",
      "feature_3 3.0000000000000044\n",
      "feature_4 4.000000000000004\n",
      "feature_5 4.999999999999991\n",
      "feature_6 6.000000000000001\n",
      "feature_7 6.9999999999999964\n",
      "feature_8 8.000000000000005\n",
      "feature_9 8.999999999999996\n",
      "feature_10 9.999999999999986\n",
      "4.373157038707131\n"
     ]
    }
   ],
   "source": [
    "for column, coef in zip(X.columns, model.coef_):\n",
    "    print(column, coef)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810352b6",
   "metadata": {},
   "source": [
    "Теперь вам необходимо реализовать класс для оптимизации коэффициентов линейной регрессии МНК.\n",
    "Подразумевается, что на вход алгоритм будет принимать следующие параметры:\n",
    "\n",
    "- 2 pandas датафрейма **samples** и **targets**, содержащих матрицу объектов и ветор ответов соответственно\n",
    "- значение **learning rate**, который корректирует длину вектора-градиента (чтобы он не взорвался)\n",
    "- значение **threshold**'а для критерия останова (когда мы считаем, что мы сошлись к оптимуму)\n",
    "- параметр **copy**, который позволяет либо делать изменения in-place в датафрейме, подающимся в класс, если изменения матрицы объектов в принципе при обучении имеются. Или же копировать объект при инициализации класса и возвращать новый объект, если требуется.\n",
    "\n",
    "Он будет состоять из следующих важных компонент-методов:\n",
    "\n",
    "- **add_constant_feature**: добавляет колонку с названием *constant* из единичек к переданному датафрейму **samples**. Это позволяет оценить свободный коэффициент $\\beta_0$.\n",
    "\n",
    "- **calculate_mse_loss**: вычисляет при текущих весах **self.beta** значение среднеквадратической ошибки.\n",
    "\n",
    "- **calculate_gradient**: вычисляет при текущих весах вектор-градиент по функционалу.\n",
    "\n",
    "- **iteration**: производит итерацию градиентного спуска, то есть обновляет веса модели, в соответствии с установленным **learning_rate = $\\eta$**: $\\beta^{(n+1)} = \\beta^{(n)} - \\eta \\cdot \\nabla Q(\\beta^{(n)})$\n",
    "\n",
    "- **learn**: производит итерации обучения до того момента, пока не сработает критерий останова обучения. В этот раз критерием останова будет следующее событие: во время крайней итерации изменение в функционале качества модели составило значение меньшее, чем **self.threshold**. Иными словами, $|Q(\\beta^{(n)}) - Q(\\beta^{(n+1)})| < threshold$.\n",
    "\n",
    "P.S. установите в **__init__** аттрибут экземпляра с названием **iteration_loss_dict**, который будет устроен следующим образом: на каждой итерации мы будем добавлять в словарь пару ключ-значение, где ключем будет номер итерации $n$, а значением - среднеквадратическая ошибка в точке $\\beta^{(n)}$. Это пригодится нам в будущем для визуализации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35843be6",
   "metadata": {},
   "source": [
    "### Hint: пример вычисления производной"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988445ce",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(a, X) = \\frac{1}{N}\\cdot\\sum_{i=1}^N (\\beta_1 \\cdot d_{i1} + ... + \\beta_n \\cdot d_{in} - y_i)^2\n",
    "$$\n",
    "\n",
    "Выше - минимизируемая функция. Она зависит от n переменных: $\\beta_1, ..., \\beta_n$. Вектор-градиент - матрица с одной строчкой, состоящей из производных 1го порядка по всем переменным.\n",
    "\n",
    "$$\n",
    "\\nabla Q(a, X) = (Q'_{\\beta_1} \\;\\;\\; Q'_{\\beta_2} \\;\\;\\; ... \\;\\;\\; Q'_{\\beta_{n-1}}  \\;\\;\\;  Q'_{\\beta_n})\n",
    "$$\n",
    "\n",
    "Пример вычисления производной по первой переменной:\n",
    "\n",
    "$$\n",
    "Q'_{\\beta_1} = \\frac{2}{N} \\cdot \\sum_{i=1}^N d_{i1} (\\beta_1 \\cdot d_{i1} + ... + \\beta_{n} \\cdot d_{in} - y_i)\n",
    "$$\n",
    "\n",
    "Скажем, для нашего датасета X, Y вычислим эту саму производную при начальных единичных коэффициентах $\\beta_{start} = (1 \\;\\;\\; 1 \\;\\;\\; ...)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f8015",
   "metadata": {},
   "source": [
    "Получим для каждого объекта в начале выражение из скобочек: \n",
    "$$\n",
    "\\beta_1 \\cdot d_{i1} + ... + \\beta_{n} \\cdot d_{in} - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d4db7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Инициализируем точку для коэффициентов в виде вектора из единичек\n",
    "initial_betas = np.ones(X.shape[1])\n",
    "\n",
    "### Получим выражение выше для каждого объекта. \n",
    "### Для этого скалярно перемножим строчки из X на наши beta\n",
    "\n",
    "scalar_value = np.dot(X, initial_betas.reshape(-1, 1)).ravel()\n",
    "scalar_value = (scalar_value - Y).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c57541",
   "metadata": {},
   "source": [
    "Теперь полученное значение для каждого объекта умножим на соответствующее значение признака $d_1$:\n",
    "\n",
    "$$\n",
    "d_{i1} \\cdot (\\beta_1 \\cdot d_{i1} + ... + \\beta_{n} \\cdot d_{in} - y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daba3af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Возьмем столбик со значениями 1 признака\n",
    "\n",
    "d_i1 = X.values[:, 0]\n",
    "\n",
    "### Умножим каждый объект на соответствующее значение признака\n",
    "scalar_value = scalar_value * d_i1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095c35ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-27.62384887912409"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Наконец, умножим все на 2 и усреднимся, \n",
    "### чтобы получить значение производной по первому параметру\n",
    "\n",
    "2 * np.mean(scalar_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba722ac2",
   "metadata": {},
   "source": [
    "### Эта логика поможем Вам при реализации класса!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5b4a8",
   "metadata": {},
   "source": [
    "learn(self)\n",
    "\n",
    "метод возвращает итоговую среднеквадратическую ошибку.\n",
    "метод итеративно вычисляет среднеквадратическую ошибку и вектор-градиент. номер итерации и MSE записываются в словарь *iteration_loss_dict*. критерий останова срабатывает тогда, когда абсолютное значение разницы двух последних MSE меньше *self.threshold*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd667639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentMse:\n",
    "    \"\"\"\n",
    "    Базовый класс для реализации градиентного спуска в задаче линейной МНК регрессии \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, samples: pd.DataFrame, targets: pd.DataFrame,\n",
    "                 learning_rate: float = 1e-3, threshold = 1e-6, copy: bool = True):\n",
    "        \"\"\"\n",
    "        :param samples: матрица объектов\n",
    "        :param targets: вектор (матрица с 1 столбцом) ответов\n",
    "        :param learning_rate: параметр learning_rate для корректировки нормы градиента\n",
    "        :param threshold: величина, меньше которой изменение в loss-функции означает \n",
    "        :param copy: копировать сэмплы или делать изменения in-place (см. add_constant_feature)\n",
    "        \"\"\"\n",
    "        if copy:\n",
    "            self.samples = samples.copy()\n",
    "        else:\n",
    "            self.samples = samples\n",
    "        self.targets = targets\n",
    "        self.learning_rate = learning_rate\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.beta = np.ones(self.samples.shape[1])\n",
    "        self.iteration_num = 0\n",
    "        self.iteration_loss_dict = {}\n",
    "        \n",
    "    def add_constant_feature(self):\n",
    "        \"\"\"\n",
    "        Метод для создания константной фичи в матрице объектов samples\n",
    "        \"\"\"\n",
    "        self.samples['constant'] = 1    \n",
    "        self.beta = np.append(self.beta, 1)\n",
    "        \n",
    "    def calculate_mse_loss(self) -> float:\n",
    "        \"\"\"\n",
    "        Метод для создания константной фичи в матрице объектов samples\n",
    "        \n",
    "        :return: среднеквадратическая ошибка при текущих весах модели : float\n",
    "        \"\"\"\n",
    "        loss = np.dot(self.samples, self.beta)  - self.targets.values\n",
    "        \n",
    "        return np.mean(loss**2)\n",
    "\n",
    "    def calculate_gradient(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Метод для вычисления вектора-градиента\n",
    "        \n",
    "        :return: вектор-градиент, т.е. массив, содержащий соответствующее количество производных по каждой переменной : np.ndarray\n",
    "        \"\"\"\n",
    "        shift = np.dot(self.samples, self.beta)  - self.targets.values\n",
    "        derivatives = 2 * np.dot(shift, self.samples) / self.samples.shape[0]\n",
    "        \n",
    "        return derivatives\n",
    "    \n",
    "    def iteration(self):\n",
    "        \"\"\"\n",
    "        Обновляем веса модели в соответствии с текущим вектором-градиентом\n",
    "        \"\"\"\n",
    "        self.beta = self.beta - self.learning_rate * self.calculate_gradient()\n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Итеративное обучение весов модели до срабатывания критерия останова\n",
    "        \"\"\"\n",
    "        previous_mse = self.calculate_mse_loss()\n",
    "        \n",
    "        self.iteration()\n",
    "        \n",
    "        next_mse = self.calculate_mse_loss()\n",
    "        \n",
    "        self.iteration_loss_dict[0] = previous_mse\n",
    "        self.iteration_loss_dict[1] = next_mse\n",
    "        \n",
    "        self.iteration_num = 1\n",
    "        \n",
    "        while abs(next_mse - previous_mse) >= self.threshold:\n",
    "            \n",
    "            previous_mse = next_mse\n",
    "            \n",
    "            self.iteration()\n",
    "            \n",
    "            next_mse = self.calculate_mse_loss()\n",
    "            \n",
    "            self.iteration_loss_dict[self.iteration_num+1] = next_mse\n",
    "            \n",
    "            self.iteration_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a31edf",
   "metadata": {},
   "source": [
    "Обучим коэффициенты линейной модели с помощью реализованного нами градиентного спуска, не забыв добавить свободную переменную. Получились ли такие же коэффициенты, как и при использовании **LinearRegression** из **sklearn**? Если нет, то почему они отличаются, на Ваш взгляд, и сильно ли?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f86135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GD = GradientDescentMse(samples=X, targets=Y)\n",
    "GD.add_constant_feature()\n",
    "GD.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba4472e3-2f03-46ba-ba55-c0433e43d0b7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 682.668858746845,\n",
       " 1: 673.1874227973016,\n",
       " 2: 663.839092118565,\n",
       " 3: 654.6219976377024,\n",
       " 4: 645.534296527588,\n",
       " 5: 636.574171838355,\n",
       " 6: 627.7398321340238,\n",
       " 7: 619.0295111342318,\n",
       " 8: 610.4414673609944,\n",
       " 9: 601.9739837904258,\n",
       " 10: 593.6253675093516,\n",
       " 11: 585.3939493767411,\n",
       " 12: 577.2780836898975,\n",
       " 13: 569.2761478553298,\n",
       " 14: 561.386542064253,\n",
       " 15: 553.6076889726401,\n",
       " 16: 545.9380333857687,\n",
       " 17: 538.3760419471971,\n",
       " 18: 530.9202028321077,\n",
       " 19: 523.5690254449574,\n",
       " 20: 516.3210401213715,\n",
       " 21: 509.17479783422556,\n",
       " 22: 502.12886990385465,\n",
       " 23: 495.18184771233024,\n",
       " 24: 488.3323424217506,\n",
       " 25: 481.5789846964864,\n",
       " 26: 474.9204244293271,\n",
       " 27: 468.3553304714725,\n",
       " 28: 461.88239036631506,\n",
       " 29: 455.50031008696254,\n",
       " 30: 449.20781377744396,\n",
       " 31: 443.0036434975512,\n",
       " 32: 436.88655897126324,\n",
       " 33: 430.8553373387024,\n",
       " 34: 424.9087729115747,\n",
       " 35: 419.0456769320434,\n",
       " 36: 413.26487733498936,\n",
       " 37: 407.5652185136088,\n",
       " 38: 401.94556108830346,\n",
       " 39: 396.4047816788155,\n",
       " 40: 390.94177267956235,\n",
       " 41: 385.55544203812633,\n",
       " 42: 380.2447130368547,\n",
       " 43: 375.0085240775263,\n",
       " 44: 369.84582846904243,\n",
       " 45: 364.75559421809857,\n",
       " 46: 359.7368038227958,\n",
       " 47: 354.78845406915025,\n",
       " 48: 349.9095558304597,\n",
       " 49: 345.0991338694882,\n",
       " 50: 340.3562266434278,\n",
       " 51: 335.6798861115993,\n",
       " 52: 331.06917754585334,\n",
       " 53: 326.5231793436335,\n",
       " 54: 322.0409828436655,\n",
       " 55: 317.62169214423255,\n",
       " 56: 313.2644239240048,\n",
       " 57: 308.96830726538263,\n",
       " 58: 304.7324834803224,\n",
       " 59: 300.55610593860655,\n",
       " 60: 296.43833989852595,\n",
       " 61: 292.37836233993966,\n",
       " 62: 288.37536179967867,\n",
       " 63: 284.4285382092615,\n",
       " 64: 280.5371027348878,\n",
       " 65: 276.70027761967947,\n",
       " 66: 272.91729602813683,\n",
       " 67: 269.1874018927788,\n",
       " 68: 265.5098497629366,\n",
       " 69: 261.88390465567124,\n",
       " 70: 258.3088419087838,\n",
       " 71: 254.78394703588987,\n",
       " 72: 251.3085155835295,\n",
       " 73: 247.88185299028288,\n",
       " 74: 244.50327444786532,\n",
       " 75: 241.17210476417185,\n",
       " 76: 237.88767822824548,\n",
       " 77: 234.64933847714133,\n",
       " 78: 231.4564383646606,\n",
       " 79: 228.30833983192744,\n",
       " 80: 225.2044137797834,\n",
       " 81: 222.14403994297373,\n",
       " 82: 219.12660676610022,\n",
       " 83: 216.15151128131617,\n",
       " 84: 213.2181589877387,\n",
       " 85: 210.32596373255416,\n",
       " 86: 207.47434759379348,\n",
       " 87: 204.66274076475307,\n",
       " 88: 201.89058144003945,\n",
       " 89: 199.15731570321313,\n",
       " 90: 196.46239741601102,\n",
       " 91: 193.80528810912395,\n",
       " 92: 191.1854568745082,\n",
       " 93: 188.60238025920867,\n",
       " 94: 186.0555421606739,\n",
       " 95: 183.54443372354,\n",
       " 96: 181.06855323786505,\n",
       " 97: 178.62740603879158,\n",
       " 98: 176.2205044076186,\n",
       " 99: 173.84736747426257,\n",
       " 100: 171.50752112108808,\n",
       " 101: 169.20049788808882,\n",
       " 102: 166.92583687940024,\n",
       " 103: 164.68308367112476,\n",
       " 104: 162.4717902204516,\n",
       " 105: 160.29151477605234,\n",
       " 106: 158.141821789735,\n",
       " 107: 156.02228182933882,\n",
       " 108: 153.93247149285173,\n",
       " 109: 151.8719733237344,\n",
       " 110: 149.84037572743267,\n",
       " 111: 147.8372728890628,\n",
       " 112: 145.86226469225252,\n",
       " 113: 143.9149566391215,\n",
       " 114: 141.9949597713857,\n",
       " 115: 140.1018905925695,\n",
       " 116: 138.2353709913101,\n",
       " 117: 136.39502816573906,\n",
       " 118: 134.58049454892532,\n",
       " 119: 132.7914077353656,\n",
       " 120: 131.0274104085068,\n",
       " 121: 129.28815026928623,\n",
       " 122: 127.5732799656751,\n",
       " 123: 125.88245702321151,\n",
       " 124: 124.21534377650902,\n",
       " 125: 122.57160730172681,\n",
       " 126: 120.9509193499882,\n",
       " 127: 119.35295628173388,\n",
       " 128: 117.77739900199757,\n",
       " 129: 116.22393289658973,\n",
       " 130: 114.6922477691781,\n",
       " 131: 113.18203777925117,\n",
       " 132: 111.69300138095348,\n",
       " 133: 110.22484126277888,\n",
       " 134: 108.77726428811168,\n",
       " 135: 107.34998143660184,\n",
       " 136: 105.94270774636371,\n",
       " 137: 104.55516225698682,\n",
       " 138: 103.18706795334602,\n",
       " 139: 101.83815171020166,\n",
       " 140: 100.50814423757693,\n",
       " 141: 99.19678002690253,\n",
       " 142: 97.90379729791773,\n",
       " 143: 96.62893794631667,\n",
       " 144: 95.3719474921302,\n",
       " 145: 94.13257502883226,\n",
       " 146: 92.91057317316118,\n",
       " 147: 91.70569801564535,\n",
       " 148: 90.51770907182372,\n",
       " 149: 89.34636923415133,\n",
       " 150: 88.1914447245801,\n",
       " 151: 87.05270504780557,\n",
       " 152: 85.92992294517002,\n",
       " 153: 84.8228743492131,\n",
       " 154: 83.73133833886031,\n",
       " 155: 82.65509709524136,\n",
       " 156: 81.5939358581281,\n",
       " 157: 80.54764288298489,\n",
       " 158: 79.51600939862168,\n",
       " 159: 78.49882956544192,\n",
       " 160: 77.49590043427642,\n",
       " 161: 76.50702190579585,\n",
       " 162: 75.53199669049265,\n",
       " 163: 74.5706302692251,\n",
       " 164: 73.62273085431552,\n",
       " 165: 72.68810935119447,\n",
       " 166: 71.76657932058383,\n",
       " 167: 70.85795694121074,\n",
       " 168: 69.96206097304518,\n",
       " 169: 69.07871272105385,\n",
       " 170: 68.20773599946263,\n",
       " 171: 67.34895709652164,\n",
       " 172: 66.50220473976421,\n",
       " 173: 65.66731006175425,\n",
       " 174: 64.84410656631445,\n",
       " 175: 64.03243009522872,\n",
       " 176: 63.23211879541207,\n",
       " 177: 62.44301308654166,\n",
       " 178: 61.664955629142376,\n",
       " 179: 60.89779129312053,\n",
       " 180: 60.14136712673913,\n",
       " 181: 59.39553232602941,\n",
       " 182: 58.66013820463126,\n",
       " 183: 57.93503816405764,\n",
       " 184: 57.220087664376194,\n",
       " 185: 56.51514419530281,\n",
       " 186: 55.82006724770093,\n",
       " 187: 55.13471828548109,\n",
       " 188: 54.45896071789526,\n",
       " 189: 53.792659872219794,\n",
       " 190: 53.135682966822564,\n",
       " 191: 52.487899084607605,\n",
       " 192: 51.84917914683331,\n",
       " 193: 51.21939588729779,\n",
       " 194: 50.59842382688701,\n",
       " 195: 49.98613924848017,\n",
       " 196: 49.38242017220758,\n",
       " 197: 48.787146331055844,\n",
       " 198: 48.20019914681571,\n",
       " 199: 47.62146170636768,\n",
       " 200: 47.05081873830038,\n",
       " 201: 46.48815658985764,\n",
       " 202: 45.93336320420903,\n",
       " 203: 45.38632809803954,\n",
       " 204: 44.84694233945416,\n",
       " 205: 44.3150985261925,\n",
       " 206: 43.79069076414949,\n",
       " 207: 43.27361464619758,\n",
       " 208: 42.76376723130648,\n",
       " 209: 42.26104702395582,\n",
       " 210: 41.76535395383721,\n",
       " 211: 41.276589355841175,\n",
       " 212: 40.794655950325115,\n",
       " 213: 40.31945782365839,\n",
       " 214: 39.8509004090405,\n",
       " 215: 39.38889046758882,\n",
       " 216: 38.9333360696916,\n",
       " 217: 38.48414657662314,\n",
       " 218: 38.041232622416906,\n",
       " 219: 37.60450609599324,\n",
       " 220: 37.17388012353801,\n",
       " 221: 36.749269051128884,\n",
       " 222: 36.33058842760518,\n",
       " 223: 35.917754987678826,\n",
       " 224: 35.5106866352818,\n",
       " 225: 35.10930242714804,\n",
       " 226: 34.713522556625506,\n",
       " 227: 34.32326833771563,\n",
       " 228: 33.93846218933703,\n",
       " 229: 33.55902761980999,\n",
       " 230: 33.184889211559074,\n",
       " 231: 32.81597260603018,\n",
       " 232: 32.45220448881972,\n",
       " 233: 32.09351257501253,\n",
       " 234: 31.739825594725538,\n",
       " 235: 31.391073278854627,\n",
       " 236: 31.047186345021426,\n",
       " 237: 30.708096483717682,\n",
       " 238: 30.373736344643977,\n",
       " 239: 30.044039523240404,\n",
       " 240: 29.71894054740639,\n",
       " 241: 29.39837486440686,\n",
       " 242: 29.082278827962444,\n",
       " 243: 28.770589685520893,\n",
       " 244: 28.463245565707147,\n",
       " 245: 28.16018546594979,\n",
       " 246: 27.861349240281132,\n",
       " 247: 27.5666775873087,\n",
       " 248: 27.276112038355507,\n",
       " 249: 26.989594945767,\n",
       " 250: 26.70706947138207,\n",
       " 251: 26.42847957516613,\n",
       " 252: 26.15377000400356,\n",
       " 253: 25.8828862806478,\n",
       " 254: 25.61577469282638,\n",
       " 255: 25.352382282499107,\n",
       " 256: 25.092656835266975,\n",
       " 257: 24.83654686992985,\n",
       " 258: 24.58400162819082,\n",
       " 259: 24.334971064505,\n",
       " 260: 24.08940583607104,\n",
       " 261: 23.847257292963043,\n",
       " 262: 23.60847746840111,\n",
       " 263: 23.37301906915851,\n",
       " 264: 23.140835466103525,\n",
       " 265: 22.911880684874163,\n",
       " 266: 22.686109396683744,\n",
       " 267: 22.463476909255668,\n",
       " 268: 22.243939157885368,\n",
       " 269: 22.027452696627975,\n",
       " 270: 21.813974689609463,\n",
       " 271: 21.603462902460052,\n",
       " 272: 21.395875693867744,\n",
       " 273: 21.191172007250476,\n",
       " 274: 20.989311362545333,\n",
       " 275: 20.790253848112922,\n",
       " 276: 20.59396011275555,\n",
       " 277: 20.400391357847287,\n",
       " 278: 20.2095093295748,\n",
       " 279: 20.021276311286957,\n",
       " 280: 19.835655115951894,\n",
       " 281: 19.65260907872009,\n",
       " 282: 19.47210204959171,\n",
       " 283: 19.29409838618707,\n",
       " 284: 19.11856294661857,\n",
       " 285: 18.94546108246262,\n",
       " 286: 18.774758631830483,\n",
       " 287: 18.606421912536135,\n",
       " 288: 18.4404177153603,\n",
       " 289: 18.276713297408925,\n",
       " 290: 18.11527637556503,\n",
       " 291: 17.95607512003241,\n",
       " 292: 17.799078147969993,\n",
       " 293: 17.64425451721573,\n",
       " 294: 17.491573720098444,\n",
       " 295: 17.341005677336653,\n",
       " 296: 17.192520732023038,\n",
       " 297: 17.046089643693463,\n",
       " 298: 16.90168358247917,\n",
       " 299: 16.759274123341125,\n",
       " 300: 16.618833240385374,\n",
       " 301: 16.480333301258167,\n",
       " 302: 16.34374706161981,\n",
       " 303: 16.209047659696115,\n",
       " 304: 16.076208610906356,\n",
       " 305: 15.945203802566654,\n",
       " 306: 15.816007488667765,\n",
       " 307: 15.688594284726161,\n",
       " 308: 15.562939162707448,\n",
       " 309: 15.439017446021015,\n",
       " 310: 15.316804804585056,\n",
       " 311: 15.19627724996082,\n",
       " 312: 15.077411130555209,\n",
       " 313: 14.960183126890723,\n",
       " 314: 14.844570246941878,\n",
       " 315: 14.730549821537046,\n",
       " 316: 14.618099499824911,\n",
       " 317: 14.507197244804557,\n",
       " 318: 14.397821328918313,\n",
       " 319: 14.289950329706569,\n",
       " 320: 14.183563125523467,\n",
       " 321: 14.07863889131288,\n",
       " 322: 13.975157094443622,\n",
       " 323: 13.873097490603215,\n",
       " 324: 13.772440119749222,\n",
       " 325: 13.673165302117523,\n",
       " 326: 13.575253634286595,\n",
       " 327: 13.478685985297052,\n",
       " 328: 13.38344349282572,\n",
       " 329: 13.289507559413384,\n",
       " 330: 13.196859848745534,\n",
       " 331: 13.105482281985335,\n",
       " 332: 13.01535703415807,\n",
       " 333: 12.92646653058643,\n",
       " 334: 12.8387934433757,\n",
       " 335: 12.752320687948501,\n",
       " 336: 12.667031419628014,\n",
       " 337: 12.582909030269336,\n",
       " 338: 12.49993714493797,\n",
       " 339: 12.418099618635141,\n",
       " 340: 12.33738053306895,\n",
       " 341: 12.257764193470935,\n",
       " 342: 12.179235125457332,\n",
       " 343: 12.101778071934383,\n",
       " 344: 12.025377990047135,\n",
       " 345: 11.950020048171043,\n",
       " 346: 11.875689622945876,\n",
       " 347: 11.802372296351207,\n",
       " 348: 11.730053852823012,\n",
       " 349: 11.658720276410753,\n",
       " 350: 11.588357747974374,\n",
       " 351: 11.51895264242064,\n",
       " 352: 11.450491525978341,\n",
       " 353: 11.382961153511678,\n",
       " 354: 11.316348465871448,\n",
       " 355: 11.25064058728338,\n",
       " 356: 11.185824822773156,\n",
       " 357: 11.121888655627592,\n",
       " 358: 11.058819744891476,\n",
       " 359: 10.996605922899521,\n",
       " 360: 10.935235192843061,\n",
       " 361: 10.874695726370788,\n",
       " 362: 10.814975861223306,\n",
       " 363: 10.75606409890081,\n",
       " 364: 10.697949102363554,\n",
       " 365: 10.640619693764602,\n",
       " 366: 10.58406485221445,\n",
       " 367: 10.528273711576974,\n",
       " 368: 10.473235558296368,\n",
       " 369: 10.4189398292546,\n",
       " 370: 10.365376109658927,\n",
       " 371: 10.312534130959088,\n",
       " 372: 10.26040376879378,\n",
       " 373: 10.208975040965893,\n",
       " 374: 10.158238105446236,\n",
       " 375: 10.108183258405287,\n",
       " 376: 10.058800932272556,\n",
       " 377: 10.010081693823237,\n",
       " 378: 9.962016242291696,\n",
       " 379: 9.914595407511444,\n",
       " 380: 9.867810148081274,\n",
       " 381: 9.8216515495571,\n",
       " 382: 9.776110822669215,\n",
       " 383: 9.731179301564579,\n",
       " 384: 9.686848442073764,\n",
       " 385: 9.643109820002282,\n",
       " 386: 9.599955129445856,\n",
       " 387: 9.557376181129381,\n",
       " 388: 9.515364900769223,\n",
       " 389: 9.473913327458446,\n",
       " 390: 9.433013612074799,\n",
       " 391: 9.392658015710998,\n",
       " 392: 9.352838908127076,\n",
       " 393: 9.313548766224482,\n",
       " 394: 9.274780172541556,\n",
       " 395: 9.236525813770168,\n",
       " 396: 9.198778479293187,\n",
       " 397: 9.161531059742488,\n",
       " 398: 9.12477654557718,\n",
       " 399: 9.088508025681815,\n",
       " 400: 9.052718685984306,\n",
       " 401: 9.017401808093215,\n",
       " 402: 8.982550767954203,\n",
       " 403: 8.94815903452531,\n",
       " 404: 8.914220168470903,\n",
       " 405: 8.880727820873883,\n",
       " 406: 8.847675731966028,\n",
       " 407: 8.815057729876182,\n",
       " 408: 8.782867729395932,\n",
       " 409: 8.751099730762725,\n",
       " 410: 8.719747818460021,\n",
       " 411: 8.688806160034282,\n",
       " 412: 8.658269004928586,\n",
       " 413: 8.628130683332664,\n",
       " 414: 8.59838560504898,\n",
       " 415: 8.56902825837485,\n",
       " 416: 8.54005320900019,\n",
       " 417: 8.511455098920752,\n",
       " 418: 8.483228645366646,\n",
       " 419: 8.455368639745847,\n",
       " 420: 8.427869946602634,\n",
       " 421: 8.400727502590556,\n",
       " 422: 8.373936315459893,\n",
       " 423: 8.347491463059294,\n",
       " 424: 8.321388092351437,\n",
       " 425: 8.29562141844253,\n",
       " 426: 8.270186723625432,\n",
       " 427: 8.245079356436214,\n",
       " 428: 8.220294730723944,\n",
       " 429: 8.195828324733576,\n",
       " 430: 8.171675680201668,\n",
       " 431: 8.147832401464852,\n",
       " 432: 8.124294154580774,\n",
       " 433: 8.101056666461414,\n",
       " 434: 8.078115724018554,\n",
       " 435: 8.055467173321242,\n",
       " 436: 8.033106918765126,\n",
       " 437: 8.0110309222534,\n",
       " 438: 7.989235202389282,\n",
       " 439: 7.967715833679824,\n",
       " 440: 7.9464689457509134,\n",
       " 441: 7.925490722573261,\n",
       " 442: 7.904777401699319,\n",
       " 443: 7.884325273510854,\n",
       " 444: 7.864130680477118,\n",
       " 445: 7.844190016423425,\n",
       " 446: 7.824499725810014,\n",
       " 447: 7.80505630302101,\n",
       " 448: 7.785856291663415,\n",
       " 449: 7.766896283875869,\n",
       " 450: 7.748172919647205,\n",
       " 451: 7.729682886144509,\n",
       " 452: 7.711422917050644,\n",
       " 453: 7.693389791911057,\n",
       " 454: 7.675580335489798,\n",
       " 455: 7.657991417134518,\n",
       " 456: 7.64061995015046,\n",
       " 457: 7.623462891183157,\n",
       " 458: 7.606517239609892,\n",
       " 459: 7.589780036939605,\n",
       " 460: 7.573248366221311,\n",
       " 461: 7.556919351460772,\n",
       " 462: 7.540790157045374,\n",
       " 463: 7.5248579871771035,\n",
       " 464: 7.5091200853134366,\n",
       " 465: 7.493573733616142,\n",
       " 466: 7.478216252407752,\n",
       " 467: 7.463044999635738,\n",
       " 468: 7.448057370344168,\n",
       " 469: 7.433250796152775,\n",
       " 470: 7.418622744743377,\n",
       " 471: 7.404170719353482,\n",
       " 472: 7.38989225827701,\n",
       " 473: 7.375784934372019,\n",
       " 474: 7.361846354575371,\n",
       " 475: 7.3480741594241765,\n",
       " 476: 7.334466022583971,\n",
       " 477: 7.321019650383553,\n",
       " 478: 7.307732781356275,\n",
       " 479: 7.294603185787875,\n",
       " 480: 7.281628665270566,\n",
       " 481: 7.268807052263439,\n",
       " 482: 7.256136209659038,\n",
       " 483: 7.243614030355995,\n",
       " 484: 7.231238436837695,\n",
       " 485: 7.219007380756819,\n",
       " 486: 7.206918842525785,\n",
       " 487: 7.19497083091286,\n",
       " 488: 7.183161382644006,\n",
       " 489: 7.1714885620102935,\n",
       " 490: 7.159950460480825,\n",
       " 491: 7.148545196321099,\n",
       " 492: 7.137270914216734,\n",
       " 493: 7.126125784902468,\n",
       " 494: 7.1151080047963875,\n",
       " 495: 7.104215795639258,\n",
       " 496: 7.09344740413897,\n",
       " 497: 7.082801101619909,\n",
       " 498: 7.072275183677333,\n",
       " 499: 7.0618679698365145,\n",
       " 500: 7.051577803216749,\n",
       " 501: 7.0414030502000164,\n",
       " 502: 7.031342100104352,\n",
       " 503: 7.021393364861748,\n",
       " 504: 7.011555278700627,\n",
       " 505: 7.001826297832742,\n",
       " 506: 6.992204900144492,\n",
       " 507: 6.982689584892557,\n",
       " 508: 6.973278872403819,\n",
       " 509: 6.963971303779492,\n",
       " 510: 6.954765440603404,\n",
       " 511: 6.945659864654385,\n",
       " 512: 6.9366531776226745,\n",
       " 513: 6.9277440008303355,\n",
       " 514: 6.918930974955582,\n",
       " 515: 6.910212759760965,\n",
       " 516: 6.901588033825408,\n",
       " 517: 6.893055494279965,\n",
       " 518: 6.884613856547327,\n",
       " 519: 6.876261854084967,\n",
       " 520: 6.867998238131881,\n",
       " 521: 6.85982177745891,\n",
       " 522: 6.851731258122556,\n",
       " 523: 6.843725483222252,\n",
       " 524: 6.83580327266104,\n",
       " 525: 6.827963462909621,\n",
       " 526: 6.820204906773712,\n",
       " 527: 6.812526473164655,\n",
       " 528: 6.804927046873277,\n",
       " 529: 6.797405528346894,\n",
       " 530: 6.789960833469473,\n",
       " 531: 6.782591893344864,\n",
       " 532: 6.775297654083083,\n",
       " 533: 6.768077076589593,\n",
       " 534: 6.7609291363575466,\n",
       " 535: 6.753852823262941,\n",
       " 536: 6.746847141362657,\n",
       " 537: 6.7399111086953365,\n",
       " 538: 6.733043757085028,\n",
       " 539: 6.726244131947638,\n",
       " 540: 6.719511292100051,\n",
       " 541: 6.712844309571965,\n",
       " 542: 6.706242269420356,\n",
       " 543: 6.699704269546536,\n",
       " 544: 6.693229420515816,\n",
       " 545: 6.68681684537965,\n",
       " 546: 6.680465679500314,\n",
       " 547: 6.674175070378052,\n",
       " 548: 6.6679441774806065,\n",
       " 549: 6.661772172075196,\n",
       " 550: 6.6556582370628075,\n",
       " 551: 6.6496015668148445,\n",
       " 552: 6.643601367012055,\n",
       " 553: 6.637656854485729,\n",
       " 554: 6.631767257061117,\n",
       " 555: 6.625931813403055,\n",
       " 556: 6.620149772863752,\n",
       " 557: 6.614420395332725,\n",
       " 558: 6.6087429510888205,\n",
       " 559: 6.603116720654345,\n",
       " 560: 6.597540994651208,\n",
       " 561: 6.59201507365911,\n",
       " 562: 6.586538268075722,\n",
       " 563: 6.581109897978812,\n",
       " 564: 6.575729292990318,\n",
       " 565: 6.570395792142333,\n",
       " 566: 6.565108743744962,\n",
       " 567: 6.559867505256046,\n",
       " 568: 6.554671443152698,\n",
       " 569: 6.5495199328046745,\n",
       " 570: 6.544412358349482,\n",
       " 571: 6.539348112569271,\n",
       " 572: 6.5343265967694535,\n",
       " 573: 6.529347220659008,\n",
       " 574: 6.524409402232477,\n",
       " 575: 6.51951256765364,\n",
       " 576: 6.51465615114077,\n",
       " 577: 6.50983959485356,\n",
       " 578: 6.505062348781586,\n",
       " 579: 6.500323870634385,\n",
       " 580: 6.495623625733033,\n",
       " 581: 6.490961086903263,\n",
       " 582: 6.486335734370106,\n",
       " 583: 6.481747055653953,\n",
       " 584: 6.477194545468153,\n",
       " 585: 6.472677705617987,\n",
       " 586: 6.468196044901089,\n",
       " 587: 6.463749079009249,\n",
       " 588: 6.4593363304316265,\n",
       " 589: 6.454957328359288,\n",
       " 590: 6.450611608591097,\n",
       " 591: 6.446298713440935,\n",
       " 592: 6.442018191646212,\n",
       " 593: 6.437769598277675,\n",
       " 594: 6.433552494650465,\n",
       " 595: 6.429366448236443,\n",
       " 596: 6.425211032577733,\n",
       " 597: 6.421085827201483,\n",
       " 598: 6.4169904175358266,\n",
       " 599: 6.412924394827012,\n",
       " 600: 6.408887356057723,\n",
       " 601: 6.404878903866511,\n",
       " 602: 6.400898646468386,\n",
       " 603: 6.396946197576502,\n",
       " 604: 6.393021176324967,\n",
       " 605: 6.389123207192706,\n",
       " 606: 6.385251919928422,\n",
       " 607: 6.381406949476591,\n",
       " 608: 6.377587935904507,\n",
       " 609: 6.3737945243303455,\n",
       " 610: 6.370026364852243,\n",
       " 611: 6.366283112478367,\n",
       " 612: 6.362564427057963,\n",
       " 613: 6.358869973213389,\n",
       " 614: 6.355199420273079,\n",
       " 615: 6.351552442205474,\n",
       " 616: 6.3479287175538595,\n",
       " 617: 6.344327929372117,\n",
       " 618: 6.340749765161415,\n",
       " 619: 6.337193916807724,\n",
       " 620: 6.333660080520283,\n",
       " 621: 6.330147956770852,\n",
       " 622: 6.3266572502338985,\n",
       " 623: 6.323187669727557,\n",
       " 624: 6.3197389281554495,\n",
       " 625: 6.3163107424493266,\n",
       " 626: 6.312902833512498,\n",
       " 627: 6.309514926164058,\n",
       " 628: 6.3061467490839185,\n",
       " 629: 6.302798034758577,\n",
       " 630: 6.299468519427675,\n",
       " 631: 6.296157943031301,\n",
       " 632: 6.2928660491580155,\n",
       " 633: 6.289592584993635,\n",
       " 634: 6.286337301270706,\n",
       " 635: 6.283099952218715,\n",
       " 636: 6.27988029551498,\n",
       " 637: 6.276678092236232,\n",
       " 638: 6.273493106810886,\n",
       " 639: 6.270325106971979,\n",
       " 640: 6.267173863710765,\n",
       " 641: 6.264039151230956,\n",
       " 642: 6.260920746903633,\n",
       " 643: 6.257818431222746,\n",
       " 644: 6.254731987761287,\n",
       " 645: 6.251661203128032,\n",
       " 646: 6.248605866924938,\n",
       " 647: 6.245565771705083,\n",
       " 648: 6.242540712931259,\n",
       " 649: 6.239530488935093,\n",
       " 650: 6.236534900876769,\n",
       " 651: 6.23355375270532,\n",
       " 652: 6.230586851119446,\n",
       " 653: 6.227634005528924,\n",
       " 654: 6.224695028016533,\n",
       " 655: 6.221769733300522,\n",
       " 656: 6.218857938697593,\n",
       " 657: 6.215959464086434,\n",
       " 658: 6.213074131871722,\n",
       " 659: 6.210201766948676,\n",
       " 660: 6.207342196668067,\n",
       " 661: 6.204495250801747,\n",
       " 662: 6.201660761508653,\n",
       " 663: 6.1988385633012895,\n",
       " 664: 6.196028493012671,\n",
       " 665: 6.193230389763746,\n",
       " 666: 6.190444094931279,\n",
       " 667: 6.187669452116156,\n",
       " 668: 6.1849063071121675,\n",
       " 669: 6.182154507875206,\n",
       " 670: 6.179413904492931,\n",
       " 671: 6.176684349154803,\n",
       " 672: 6.173965696122585,\n",
       " 673: 6.171257801701256,\n",
       " 674: 6.168560524210301,\n",
       " 675: 6.165873723955441,\n",
       " 676: 6.1631972632007255,\n",
       " 677: 6.160531006141049,\n",
       " 678: 6.157874818875041,\n",
       " 679: 6.155228569378318,\n",
       " 680: 6.1525921274771385,\n",
       " 681: 6.149965364822421,\n",
       " 682: 6.147348154864115,\n",
       " 683: 6.144740372825944,\n",
       " 684: 6.142141895680501,\n",
       " 685: 6.139552602124681,\n",
       " 686: 6.1369723725554834,\n",
       " 687: 6.13440108904613,\n",
       " 688: 6.131838635322534,\n",
       " 689: 6.129284896740096,\n",
       " 690: 6.126739760260812,\n",
       " 691: 6.124203114430724,\n",
       " 692: 6.121674849357676,\n",
       " 693: 6.1191548566893905,\n",
       " 694: 6.116643029591827,\n",
       " 695: 6.114139262727883,\n",
       " 696: 6.111643452236372,\n",
       " 697: 6.1091554957112875,\n",
       " 698: 6.10667529218139,\n",
       " 699: 6.1042027420900355,\n",
       " 700: 6.101737747275339,\n",
       " 701: 6.099280210950572,\n",
       " 702: 6.096830037684864,\n",
       " 703: 6.094387133384151,\n",
       " 704: 6.091951405272423,\n",
       " 705: 6.089522761873203,\n",
       " 706: 6.087101112991309,\n",
       " 707: 6.0846863696948565,\n",
       " 708: 6.0822784442975175,\n",
       " 709: 6.079877250341042,\n",
       " 710: 6.077482702578009,\n",
       " 711: 6.075094716954822,\n",
       " 712: 6.072713210594949,\n",
       " 713: 6.070338101782392,\n",
       " 714: 6.0679693099454015,\n",
       " 715: 6.065606755640402,\n",
       " 716: 6.0632503605361405,\n",
       " 717: 6.060900047398097,\n",
       " 718: 6.058555740073054,\n",
       " 719: 6.056217363473938,\n",
       " 720: 6.053884843564831,\n",
       " 721: 6.051558107346225,\n",
       " 722: 6.049237082840466,\n",
       " 723: 6.046921699077396,\n",
       " 724: 6.044611886080229,\n",
       " 725: 6.042307574851567,\n",
       " 726: 6.0400086973596965,\n",
       " 727: 6.037715186524986,\n",
       " 728: 6.0354269762065424,\n",
       " 729: 6.033144001189019,\n",
       " 730: 6.030866197169628,\n",
       " 731: 6.02859350074532,\n",
       " 732: 6.026325849400156,\n",
       " 733: 6.024063181492853,\n",
       " 734: 6.021805436244492,\n",
       " 735: 6.019552553726425,\n",
       " 736: 6.017304474848323,\n",
       " 737: 6.015061141346419,\n",
       " 738: 6.012822495771888,\n",
       " 739: 6.0105884814794095,\n",
       " 740: 6.0083590426159,\n",
       " 741: 6.006134124109363,\n",
       " 742: 6.0039136716579415,\n",
       " 743: 6.001697631719102,\n",
       " 744: 5.999485951498967,\n",
       " 745: 5.9972785789418035,\n",
       " 746: 5.995075462719671,\n",
       " 747: 5.992876552222183,\n",
       " 748: 5.990681797546452,\n",
       " 749: 5.9884911494871425,\n",
       " 750: 5.98630455952668,\n",
       " 751: 5.984121979825603,\n",
       " 752: 5.981943363213024,\n",
       " 753: 5.979768663177263,\n",
       " 754: 5.977597833856576,\n",
       " 755: 5.975430830030037,\n",
       " 756: 5.973267607108542,\n",
       " 757: 5.971108121125932,\n",
       " 758: 5.968952328730251,\n",
       " 759: 5.966800187175118,\n",
       " 760: 5.964651654311239,\n",
       " 761: 5.962506688578,\n",
       " 762: 5.96036524899522,\n",
       " 763: 5.958227295154992,\n",
       " 764: 5.956092787213659,\n",
       " 765: 5.953961685883882,\n",
       " 766: 5.951833952426829,\n",
       " 767: 5.9497095486444795,\n",
       " 768: 5.947588436872024,\n",
       " 769: 5.945470579970396,\n",
       " 770: 5.9433559413188615,\n",
       " 771: 5.94124448480777,\n",
       " 772: 5.939136174831354,\n",
       " 773: 5.9370309762806786,\n",
       " 774: 5.934928854536645,\n",
       " 775: 5.932829775463126,\n",
       " 776: 5.930733705400183,\n",
       " 777: 5.928640611157376,\n",
       " 778: 5.926550460007178,\n",
       " 779: 5.92446321967847,\n",
       " 780: 5.922378858350133,\n",
       " 781: 5.920297344644741,\n",
       " 782: 5.918218647622319,\n",
       " 783: 5.916142736774203,\n",
       " 784: 5.914069582016993,\n",
       " 785: 5.91199915368657,\n",
       " 786: 5.909931422532229,\n",
       " 787: 5.9078663597108445,\n",
       " 788: 5.905803936781186,\n",
       " 789: 5.903744125698239,\n",
       " 790: 5.901686898807667,\n",
       " 791: 5.899632228840322,\n",
       " 792: 5.897580088906829,\n",
       " 793: 5.895530452492255,\n",
       " 794: 5.893483293450863,\n",
       " 795: 5.891438586000921,\n",
       " 796: 5.889396304719588,\n",
       " 797: 5.8873564245378835,\n",
       " 798: 5.885318920735715,\n",
       " 799: 5.883283768936983,\n",
       " 800: 5.881250945104744,\n",
       " 801: 5.879220425536463,\n",
       " 802: 5.877192186859306,\n",
       " 803: 5.87516620602551,\n",
       " 804: 5.873142460307838,\n",
       " 805: 5.8711209272950615,\n",
       " 806: 5.869101584887524,\n",
       " 807: 5.8670844112927805,\n",
       " 808: 5.86506938502127,\n",
       " 809: 5.8630564848820725,\n",
       " 810: 5.861045689978713,\n",
       " 811: 5.859036979705032,\n",
       " 812: 5.857030333741104,\n",
       " 813: 5.855025732049222,\n",
       " 814: 5.853023154869932,\n",
       " 815: 5.851022582718141,\n",
       " 816: 5.849023996379245,\n",
       " 817: 5.847027376905346,\n",
       " 818: 5.845032705611504,\n",
       " 819: 5.843039964072046,\n",
       " 820: 5.841049134116921,\n",
       " 821: 5.839060197828123,\n",
       " 822: 5.837073137536139,\n",
       " 823: 5.8350879358164764,\n",
       " 824: 5.833104575486208,\n",
       " 825: 5.831123039600592,\n",
       " 826: 5.829143311449724,\n",
       " 827: 5.827165374555239,\n",
       " 828: 5.825189212667067,\n",
       " 829: 5.823214809760217,\n",
       " 830: 5.821242150031634,\n",
       " 831: 5.819271217897067,\n",
       " 832: 5.817301997988008,\n",
       " 833: 5.815334475148664,\n",
       " 834: 5.813368634432965,\n",
       " 835: 5.811404461101621,\n",
       " 836: 5.809441940619225,\n",
       " 837: 5.807481058651385,\n",
       " 838: 5.805521801061908,\n",
       " 839: 5.803564153910009,\n",
       " 840: 5.801608103447584,\n",
       " 841: 5.799653636116483,\n",
       " 842: 5.797700738545867,\n",
       " 843: 5.795749397549563,\n",
       " 844: 5.793799600123475,\n",
       " 845: 5.791851333443037,\n",
       " 846: 5.789904584860679,\n",
       " 847: 5.787959341903358,\n",
       " 848: 5.78601559227009,\n",
       " 849: 5.784073323829556,\n",
       " 850: 5.782132524617702,\n",
       " 851: 5.780193182835406,\n",
       " 852: 5.778255286846152,\n",
       " 853: 5.776318825173754,\n",
       " 854: 5.774383786500104,\n",
       " 855: 5.772450159662948,\n",
       " 856: 5.770517933653714,\n",
       " 857: 5.768587097615335,\n",
       " 858: 5.7666576408401395,\n",
       " 859: 5.764729552767744,\n",
       " 860: 5.762802822982989,\n",
       " 861: 5.7608774412139026,\n",
       " 862: 5.75895339732969,\n",
       " 863: 5.75703068133875,\n",
       " 864: 5.755109283386729,\n",
       " 865: 5.75318919375458,\n",
       " 866: 5.75127040285668,\n",
       " 867: 5.749352901238949,\n",
       " 868: 5.747436679577,\n",
       " 869: 5.745521728674328,\n",
       " 870: 5.743608039460517,\n",
       " 871: 5.741695602989454,\n",
       " 872: 5.739784410437603,\n",
       " 873: 5.737874453102275,\n",
       " 874: 5.735965722399937,\n",
       " 875: 5.734058209864533,\n",
       " 876: 5.732151907145842,\n",
       " 877: 5.730246806007851,\n",
       " 878: 5.72834289832715,\n",
       " 879: 5.726440176091354,\n",
       " 880: 5.724538631397547,\n",
       " 881: 5.722638256450741,\n",
       " 882: 5.720739043562363,\n",
       " 883: 5.718840985148765,\n",
       " 884: 5.716944073729747,\n",
       " 885: 5.71504830192711,\n",
       " 886: 5.713153662463224,\n",
       " 887: 5.71126014815961,\n",
       " 888: 5.7093677519355595,\n",
       " 889: 5.707476466806755,\n",
       " 890: 5.705586285883922,\n",
       " 891: 5.7036972023714885,\n",
       " 892: 5.701809209566281,\n",
       " 893: 5.699922300856218,\n",
       " 894: 5.69803646971904,\n",
       " 895: 5.696151709721038,\n",
       " 896: 5.694268014515831,\n",
       " 897: 5.692385377843115,\n",
       " 898: 5.69050379352748,\n",
       " 899: 5.688623255477199,\n",
       " 900: 5.686743757683067,\n",
       " 901: 5.684865294217239,\n",
       " 902: 5.682987859232087,\n",
       " 903: 5.681111446959076,\n",
       " 904: 5.679236051707658,\n",
       " 905: 5.677361667864164,\n",
       " 906: 5.6754882898907555,\n",
       " 907: 5.673615912324322,\n",
       " 908: 5.671744529775459,\n",
       " 909: 5.669874136927432,\n",
       " 910: 5.668004728535143,\n",
       " 911: 5.666136299424146,\n",
       " 912: 5.66426884448963,\n",
       " 913: 5.662402358695472,\n",
       " 914: 5.660536837073247,\n",
       " 915: 5.658672274721299,\n",
       " 916: 5.656808666803787,\n",
       " 917: 5.654946008549777,\n",
       " 918: 5.65308429525232,\n",
       " 919: 5.651223522267564,\n",
       " 920: 5.649363685013859,\n",
       " 921: 5.647504778970899,\n",
       " 922: 5.645646799678841,\n",
       " 923: 5.643789742737474,\n",
       " 924: 5.6419336038053824,\n",
       " 925: 5.640078378599112,\n",
       " 926: 5.638224062892358,\n",
       " 927: 5.6363706525151755,\n",
       " 928: 5.634518143353171,\n",
       " 929: 5.632666531346742,\n",
       " 930: 5.6308158124902965,\n",
       " 931: 5.628965982831498,\n",
       " 932: 5.62711703847052,\n",
       " 933: 5.625268975559314,\n",
       " 934: 5.623421790300869,\n",
       " 935: 5.621575478948513,\n",
       " 936: 5.619730037805201,\n",
       " 937: 5.617885463222812,\n",
       " 938: 5.616041751601478,\n",
       " 939: 5.614198899388887,\n",
       " 940: 5.6123569030796405,\n",
       " 941: 5.610515759214575,\n",
       " 942: 5.608675464380122,\n",
       " 943: 5.606836015207672,\n",
       " 944: 5.604997408372939,\n",
       " 945: 5.603159640595343,\n",
       " 946: 5.601322708637395,\n",
       " 947: 5.599486609304087,\n",
       " 948: 5.597651339442316,\n",
       " 949: 5.595816895940274,\n",
       " 950: 5.593983275726877,\n",
       " 951: 5.592150475771205,\n",
       " 952: 5.590318493081921,\n",
       " 953: 5.588487324706727,\n",
       " 954: 5.586656967731824,\n",
       " 955: 5.584827419281351,\n",
       " 956: 5.582998676516867,\n",
       " 957: 5.581170736636839,\n",
       " 958: 5.579343596876094,\n",
       " 959: 5.577517254505335,\n",
       " 960: 5.575691706830632,\n",
       " 961: 5.573866951192914,\n",
       " 962: 5.5720429849675055,\n",
       " 963: 5.570219805563619,\n",
       " 964: 5.568397410423892,\n",
       " 965: 5.566575797023923,\n",
       " 966: 5.564754962871795,\n",
       " 967: 5.562934905507643,\n",
       " 968: 5.56111562250318,\n",
       " 969: 5.559297111461272,\n",
       " 970: 5.5574793700154945,\n",
       " 971: 5.555662395829702,\n",
       " 972: 5.553846186597609,\n",
       " 973: 5.552030740042367,\n",
       " 974: 5.550216053916154,\n",
       " 975: 5.548402125999764,\n",
       " 976: 5.546588954102213,\n",
       " 977: 5.544776536060339,\n",
       " 978: 5.542964869738414,\n",
       " 979: 5.54115395302776,\n",
       " 980: 5.53934378384637,\n",
       " 981: 5.537534360138531,\n",
       " 982: 5.535725679874466,\n",
       " 983: 5.533917741049953,\n",
       " 984: 5.532110541685987,\n",
       " 985: 5.530304079828417,\n",
       " 986: 5.528498353547593,\n",
       " 987: 5.5266933609380375,\n",
       " 988: 5.524889100118089,\n",
       " 989: 5.523085569229588,\n",
       " 990: 5.521282766437532,\n",
       " 991: 5.519480689929757,\n",
       " 992: 5.517679337916619,\n",
       " 993: 5.515878708630684,\n",
       " 994: 5.514078800326402,\n",
       " 995: 5.512279611279817,\n",
       " 996: 5.510481139788252,\n",
       " 997: 5.508683384170023,\n",
       " 998: 5.5068863427641395,\n",
       " 999: 5.505090013930006,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GD.iteration_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cad96ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса модели при переменных d0, d1, ..., d10 равны соответственно: \n",
      "\n",
      "[0.88733305 1.90100713 2.88063607 3.87662612 4.89623507 5.89126182\n",
      " 6.89254811 7.90311947 8.87580109 9.86109585 4.94854733]\n"
     ]
    }
   ],
   "source": [
    "print('Веса модели при переменных d0, d1, ..., d10 равны соответственно: \\n\\n' + str(GD.beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b736b8",
   "metadata": {},
   "source": [
    "Попробуйте теперь изменить значения **learning_rate** и/или **threshold**. Например, установите длину шага $\\eta = 1$. Что произошло и почему такое возможно?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ab9388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/tmp/ipykernel_220/4100436744.py:76: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  while abs(next_mse - previous_mse) >= self.threshold:\n"
     ]
    }
   ],
   "source": [
    "### Your code is here\n",
    "GD_new = GradientDescentMse(samples=X, targets=Y, learning_rate=1)\n",
    "GD_new.add_constant_feature()\n",
    "GD_new.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "100d55e8-8321-434e-b335-eecadaea3237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса модели при переменных d0, d1, ..., d10, d0 равны соответственно: \n",
      "\n",
      "[9.40188501e+152 9.18141651e+152 8.60666268e+152 8.65906169e+152\n",
      " 8.84957221e+152 9.26558544e+152 9.00779348e+152 9.33707984e+152\n",
      " 8.88112792e+152 8.93870956e+152 1.77388075e+153]\n"
     ]
    }
   ],
   "source": [
    "print('Веса модели при переменных d0, d1, ..., d10, d0 равны соответственно: \\n\\n' + str(GD_new.beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46812339",
   "metadata": {},
   "source": [
    "В машинном обучении зачастую исследуют так называемые **траектории обучения** (или **learning paths**). Это графики, показывающие, как во время обучения при каждой следующей итерации изменялось значение минимизируемого функционала. Постройте такие траектории для различных **learning rate**'ов и **threshold**'ов. Советуем использовать для этого разобранный на занятиях **add_subplot** метод. \n",
    "\n",
    "Возьмите следующие **threshold**'ы: 1e-2, 1e-3, 1e-4, 1e-5\n",
    "\n",
    "И следующие значения **learning rate**'а: 1e-1, 5e-2, 1e-2, 5e-3, 1e-3\n",
    "\n",
    "У вас должен получиться примерно такой график (см. ниже, значения среднеквадратической ошибки мы намеренно замазали оранжевыми квадратиками, чтобы не спойлерить вам результаты).\n",
    "\n",
    "Как и подобает хорошим Data Scientist'ам, не забывайте подписывать графики, оси, а так же делать элементы ваших визуализаций читаемыми и видимыми. Советуем пересмотреть методы и параметры форматирования из лекции.\n",
    "\n",
    "При какой комбинации **threshold** - **learning rate** из возможных предложенных выше, получается достигнуть меньшего значения нашей минимизируемой функции? Запишите каждой из значений в легенде на графиках.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fdc89d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Your code is here\n",
    "fig = plt.figure()\n",
    "\n",
    "fig.set_size_inches(13, 10)\n",
    "\n",
    "rates = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "thresholds = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    thresh = thresholds[i]\n",
    "    \n",
    "    ax_ = fig.add_subplot(2, 2, i+1)\n",
    "    \n",
    "    Q_values = []\n",
    "    \n",
    "    for lr in rates:\n",
    "        GD = GradientDescentMse(samples=X, targets=Y, \n",
    "                                learning_rate=lr, threshold=thresh)\n",
    "        GD.add_constant_feature()\n",
    "        GD.learn()\n",
    "\n",
    "        learning_path = GD.iteration_loss_dict\n",
    "\n",
    "        plt.plot(learning_path.keys(), learning_path.values())\n",
    "        plt.title(f'Threshold = {thresh}')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.xlim(0, 2000)\n",
    "        \n",
    "        Q_values.append(str(round(list(learning_path.values())[-1], ndigits=4)))\n",
    "    \n",
    "    plt.ylabel('Среднеквадратическая ошибка')\n",
    "    plt.xlabel('Номер итерации')\n",
    "    plt.legend([f'Learning rate equals to {rates[i]}' + ' with Q = ' + Q_values[i] for i in range(len(rates))])\n",
    "\n",
    "fig.tight_layout() \n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
